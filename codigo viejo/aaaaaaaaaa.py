# -*- coding: utf-8 -*-
"""AAAAAAAAAA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Qslh46wYCS2W2UVdf2GtSP5ca7JcsTvC
"""

import sys
import numpy as np
import os
from tensorflow.python.keras.preprocessing.image import ImageDataGenerator
from tensorflow.python.keras import optimizers
from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dropout, Flatten, Dense, Activation
from tensorflow.python.keras.layers import  Convolution2D, MaxPooling2D
from tensorflow.python.keras import backend as K
import tensorflow as tf
from keras.models import load_model
from keras.preprocessing.image import load_img, img_to_array
K.clear_session()
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, f1_score, roc_curve, precision_score, recall_score, accuracy_score, roc_auc_score
from sklearn import metrics
from mlxtend.plotting import plot_confusion_matrix

epocas= 10
longitud, altura = 128, 128
batch_size = 16
filtrosConv1 = 12
filtrosConv2 = 12
tamano_filtro1 = (3, 3)
tamano_filtro2 = (2, 2)
tamano_filtro3 = (2, 2)
tamano_pool = (2, 2)
clases = 4

data_entrenamiento = 'sample_data/dataset/train'
data_validacion = 'sample_data/dataset/test'

"""# Nueva secci√≥n"""

train = ImageDataGenerator(rescale=1. / 255,rotation_range=15,zoom_range=0.1,brightness_range=(0.2,0.8),shear_range=5)
validation = ImageDataGenerator(rescale=1. / 255,rotation_range=15,zoom_range=0.1,brightness_range=(0.2,0.8),shear_range=5)

train_dataset = train.flow_from_directory(
    data_entrenamiento,
    target_size=(altura, longitud),
    batch_size=batch_size,
    class_mode='categorical')

validation_dataset = validation.flow_from_directory(
    data_validacion,
    target_size=(altura, longitud),
    batch_size=batch_size,
    class_mode='categorical')

train_dataset.class_indices

cnn = Sequential()
cnn.add(Convolution2D(filtrosConv1, tamano_filtro1, input_shape=(longitud, altura, 3), activation='relu'))
cnn.add(MaxPooling2D(pool_size=tamano_pool))

cnn.add(Convolution2D(filtrosConv2, tamano_filtro2))
cnn.add(MaxPooling2D(pool_size=tamano_pool))

cnn.add(Flatten())
cnn.add(Dense(256, activation='relu'))
cnn.add(Dropout(.2))
cnn.add(Dense(clases, activation='softmax'))

cnn.compile(loss='categorical_crossentropy',
            optimizer='adam',
            metrics=['accuracy', tf.keras.metrics.Recall()])


history = cnn.fit(
    train_dataset,
    epochs=epocas,
    validation_data=validation_dataset)

target_dir = './modelo/'
if not os.path.exists(target_dir):
  os.mkdir(target_dir)
cnn.save('./modelo/modelo.h5')



"""# **GRAFICAS**"""

loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, 'y', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
plt.plot(epochs, acc, 'y', label='Training acc')
plt.plot(epochs, val_acc, 'r', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""
# **Probar Modelo**"""

dim = (128,128)
model = load_model('modelo/modelo.h5')


blinking = 0
center = 1
left = 2
right = 3

def predict(file):
    x = load_img(file, target_size=dim)
    x = img_to_array(x)
    x = np.expand_dims(x, axis=0)
    array = model.predict(x)
    result = array[0]
    answer = np.argmax(result)
    return answer



aciertos = np.array([0,0,0,0])
total = np.array([0,0,0,0])

opciones = ['blinking','center','left','right']

i = 0
for opcion in opciones:
    dir = 'fotos/' + opcion
    for file in os.listdir(dir):
        if (predict(dir + '/' + file) == i):
            aciertos[i] = aciertos[i] + 1
        total[i] = total[i] + 1
    i = i + 1

for j in range(4):
    print("Tasa de acierto Categoria  ",j, aciertos[j] / total[j])